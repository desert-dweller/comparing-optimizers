# Benchmarking Classical Optimization Algorithms for Non-Convex Landscapes

**Author:** Karim El Tohamy  
**Course:** Scientific Computing (CIT644)  

## ðŸ“Œ Overview

This project implements a comprehensive library of numerical optimization algorithms from first principles and benchmarks them against industry-standard implementations (`SciPy` and `PyTorch`). 

The core innovation is a **"Black Box" Automatic Numerical Differentiation Engine** that calculates gradients and Hessians using Central Finite Difference, removing the need for manual derivative formulas.

### Algorithms Implemented (From Scratch)
1.  **Adam:** Adaptive Moment Estimation (First-Order).
2.  **Newton's Method:** Exact Hessian Inversion (Second-Order).
3.  **BFGS:** Quasi-Newton Approximation.
4.  **Nelder-Mead:** Gradient-Free Simplex Method.

---

## ðŸ“‚ Project Structure

```text
â”œâ”€â”€ optimizers.py       # Core library containing the Optimizer classes and Auto-Diff engine
â”œâ”€â”€ benchmarks.py       # Suite of 7 pathological test functions (Rosenbrock, Himmelblau, etc.)
â”œâ”€â”€ main.py             # Main runner: executes experiments, logs data to JSON, saves CSV summaries
â”œâ”€â”€ plot_results.py     # Visualization: generates Convergence and Trajectory plots from logs
â”œâ”€â”€ compare_results.py  # Analysis: calculates exact error metrics vs. SciPy/PyTorch
â”œâ”€â”€ logs/               # (Generated) Raw JSON logs of optimization history
â”œâ”€â”€ plots/              # (Generated) High-res PNG plots
â””â”€â”€ results/            # (Generated) Summary CSV files